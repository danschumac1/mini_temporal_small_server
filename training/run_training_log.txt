nohup: ignoring input
Gemma's activation function should be approximate GeLU and not exact GeLU.
Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.
Using device: cuda
GPU Name: NVIDIA RTX A6000
ARG PARSE DONE

DATALOADED DONE
DATALOADERS DONE
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.42it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.49it/s]
MODEL CONFIG DONE
PEFT DONE
FREEZING DONE
OPTIMSCHEDULER DONE
SAMPLING DONE
VAL DONE
  0%|          | 0/66 [00:00<?, ?it/s]  2%|▏         | 1/66 [00:02<02:54,  2.69s/it]  3%|▎         | 2/66 [00:23<14:07, 13.24s/it]  5%|▍         | 3/66 [00:44<17:32, 16.71s/it]  6%|▌         | 4/66 [01:05<19:01, 18.41s/it]  8%|▊         | 5/66 [01:26<19:43, 19.40s/it]  9%|▉         | 6/66 [01:47<20:03, 20.06s/it] 11%|█         | 7/66 [02:09<20:09, 20.51s/it] 12%|█▏        | 8/66 [02:30<20:08, 20.84s/it] 14%|█▎        | 9/66 [02:52<20:01, 21.08s/it] 15%|█▌        | 10/66 [03:13<19:49, 21.25s/it] 17%|█▋        | 11/66 [03:35<19:36, 21.39s/it] 18%|█▊        | 12/66 [04:14<24:07, 26.81s/it] 20%|█▉        | 13/66 [04:36<22:19, 25.28s/it] 21%|██        | 14/66 [04:58<20:59, 24.22s/it] 23%|██▎       | 15/66 [05:20<19:57, 23.47s/it] 24%|██▍       | 16/66 [05:41<19:07, 22.95s/it] 26%|██▌       | 17/66 [06:03<18:26, 22.58s/it] 27%|██▋       | 18/66 [06:25<17:51, 22.33s/it] 29%|██▉       | 19/66 [06:47<17:21, 22.15s/it] 30%|███       | 20/66 [07:08<16:52, 22.02s/it] 32%|███▏      | 21/66 [07:30<16:28, 21.96s/it] 33%|███▎      | 22/66 [07:52<16:04, 21.91s/it]